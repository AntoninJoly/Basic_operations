{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label\tDescription\n",
    "Machine Learning models are composed of two different types of parameters:\n",
    "Hyperparameters = are all the parameters which can be arbitrarily set by the user before starting training (eg. number of estimators in Random Forest).\n",
    "Model parameters = are instead learned during the model training (eg. weights in Neural Networks, Linear Regression).\n",
    "The model parameters define how to use input data to get the desired output and are learned at training time. Instead, Hyperparameters determine how our model is structured in the first place.\n",
    "Machine Learning models tuning is a type of optimization problem. We have a set of hyperparameters and we aim to find the right combination of their values which can help us to find either the minimum (eg. loss) or the maximum (eg. accuracy) of a function (Figure 1).\n",
    "This can be particularly important when comparing how different Machine Learning models performs on a dataset. In fact, it would be unfair for example to compare an SVM model with the best Hyperparameters against a Random Forest model which has not been optimized.\n",
    "In this post, the following approaches to Hyperparameter optimization will be explained:\n",
    "Manual Search\n",
    "Random Search\n",
    "Grid Search\n",
    "Automated Hyperparameter Tuning (Bayesian Optimization, Genetic Algorithms)\n",
    "Artificial Neural Networks (ANNs) Tuning\n",
    "\n",
    "Figure 1: ML Optimization Workflow [1]\n",
    "In order to demonstrate how to perform Hyperparameters Optimization in Python, I decided to perform a complete Data Analysis of the Credit Card Fraud Detection Kaggle Dataset. Our objective in this article will be to correctly classify which credit card transactions should be labelled as fraudulent or genuine (binary classification). This Dataset has been anonymized before being distributed, therefore, the meaning of most of the features has not been disclosed.\n",
    "In this case, I decided to use just a subset of the dataset, in order to speed up training times and make sure to achieve a perfect balance between the two different classes. Additionally, just a limited amount of features has been used to make the optimization tasks more challenging. The final dataset is shown in the figure below (Figure 2).\n",
    "\n",
    "\n",
    "In statistics, hyperparameter is a parameter from a prior distribution; it captures the prior belief before data is observed.\n",
    "In any machine learning algorithm, these parameters need to be initialized before training a model.\n",
    "Model parameters vs Hyperparameters\n",
    "Model parameters are the properties of training data that will learn on its own during training by the classifier or other ML model. For example,\n",
    "Weights and Biases\n",
    "Split points in Decision Tree\n",
    "\n",
    "Figure 2: Hyperparameters vs model parameters → Source\n",
    "Model Hyperparameters are the properties that govern the entire training process. The below are the variables usually configure before training a model.\n",
    "Learning Rate\n",
    "Number of Epochs\n",
    "Hidden Layers\n",
    "Hidden Units\n",
    "Activations Functions\n",
    "Why are Hyperparameters essential?\n",
    "Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant impact on the performance of the model is being trained.\n",
    "“A good choice of hyperparameters can really make an algorithm shine”.\n",
    "Choosing appropriate hyperparameters plays a crucial role in the success of our neural network architecture. Since it makes a huge impact on the learned model. For example, if the learning rate is too low, the model will miss the important patterns in the data. If it is high, it may have collisions.\n",
    "Choosing good hyperparameters gives two benefits:\n",
    "Efficiently search the space of possible hyperparameters\n",
    "Easy to manage a large set of experiments for hyperparameter tuning.\n",
    "Hyperparameters Optimisation Techniques\n",
    "The process of finding most optimal hyperparameters in machine learning is called hyperparameter optimisation.\n",
    "Common algorithms include:\n",
    "Grid Search\n",
    "Random Search\n",
    "Bayesian Optimisation\n",
    "Grid Search\n",
    "Grid search is a very traditional technique for implementing hyperparameters. It brute force all combinations. Grid search requires to create two set of hyperparameters.\n",
    "Learning Rate\n",
    "Number of Layers\n",
    "Grid search trains the algorithm for all combinations by using the two set of hyperparameters (learning rate and number of layers) and measures the performance using “Cross Validation” technique. This validation technique gives assurance that our trained model got most of the patterns from the dataset. One of the best methods to do validation by using “K-Fold Cross Validation” which helps to provide ample data for training the model and ample data for validations.\n",
    "\n",
    "Figure 3: Grid Search → Source\n",
    "The Grid search method is a simpler algorithm to use but it suffers if data have high dimensional space called the curse of dimensionality.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit.    Maecenas quis nunc pulvinar urna faucibus tincidunt ut vestibulum ligula. Sed placerat sollicitudin erat, quis dapibus nibh tempor non. \n",
    "      <br/>\n",
    "    \n",
    "Id | Syntax      | Description \n",
    "--|:---------:|:-----------:\n",
    "1|Header      | Something  here\n",
    "2|More here   | Text\n",
    "    \n",
    "  </div>\n",
    "    \n",
    "  <div class=\"column\">\n",
    "    \n",
    "| Label |    Cloth    |\n",
    "|-------|-------------|\n",
    "|   0   | T-shirt/top |\n",
    "|   1   |  Trouser    |\n",
    "|   2   |  Pullover   |\n",
    "|   3   |   Dress     |\n",
    "|   4   |    Coat     |\n",
    "|   5   |   Sandal    |\n",
    "|   6   |    Shirt    |\n",
    "|   7   |   Sneaker   |\n",
    "|   8   |     Bag     |\n",
    "|   9   | Ankle boot  |\n",
    "    \n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit.    Maecenas quis nunc pulvinar urna faucibus tincidunt ut vestibulum ligula. Sed placerat sollicitudin erat, quis dapibus nibh tempor non. \n",
    "      <br/>\n",
    "    \n",
    "Id | Syntax      | Description \n",
    "--|:---------:|:-----------:\n",
    "1|Header      | Something  here\n",
    "2|More here   | Text\n",
    "    \n",
    "  </div>\n",
    "    \n",
    "  <div class=\"column\">\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas quis nunc pulvinar urna faucibus tincidunt ut vestibulum ligula. Sed placerat sollicitudin erat, quis dapibus nibh tempor non. \n",
    "  <br/>\n",
    "    \n",
    "  $$\n",
    "  \\begin{align}\n",
    "  {x} & = \\sigma(y-x) \\tag{3-1}\\\\\n",
    "  {y} & = \\rho x - y - xz \\tag{3-2}\\\\\n",
    "  {x+y+z} & = -\\beta z + xy \\tag{3-3}\n",
    "  \\end{align}\n",
    "  $$\n",
    "    \n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "%%html\n",
    "<style>\n",
    "    @media print { \n",
    "        * {\n",
    "             box-sizing: border-box;\n",
    "          }\n",
    "        .row {\n",
    "             display: flex;\n",
    "         }\n",
    "       /* Create two equal columns that sits next to each other */\n",
    "       .column {\n",
    "          flex: 50%;\n",
    "          padding: 10px;\n",
    "  \n",
    "        }\n",
    "        \n",
    "        div.input {\n",
    "          display: none;\n",
    "          padding: 0;\n",
    "        }\n",
    "        div.output_prompt {\n",
    "          display: none;\n",
    "          padding: 0;\n",
    "        }\n",
    "        div.text_cell_render {\n",
    "          padding: 1pt;\n",
    "        }\n",
    "        div#notebook p,\n",
    "        div#notebook,\n",
    "        div#notebook li,\n",
    "        p {\n",
    "          font-size: 10pt;\n",
    "          line-height: 115%;\n",
    "          margin: 0;\n",
    "        }\n",
    "        .rendered_html h1,\n",
    "        .rendered_html h1:first-child {\n",
    "          font-size: 10pt;\n",
    "          margin: 3pt 0;\n",
    "        }\n",
    "       .rendered_html h2,\n",
    "       .rendered_html h2:first-child {\n",
    "          font-size: 10pt;\n",
    "          margin: 3pt 0;\n",
    "       }\n",
    "       .rendered_html h3,\n",
    "       .rendered_html h3:first-child {\n",
    "         font-size: 10pt;\n",
    "         margin: 3pt 0;\n",
    "       }\n",
    "       div.output_subarea {\n",
    "         padding: 0;\n",
    "       }\n",
    "       div.input_prompt{\n",
    "         display: none;\n",
    "         padding: 0;\n",
    "      }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dense, Dropout, convolutional, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import talos as ta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\", \"\", DeprecationWarning, \"\", 0)\n",
    "\n",
    "# load training data and do basic data normalization\n",
    "(X_train, y_train), (X_val, y_val) = fashion_mnist.load_data()\n",
    "\n",
    "# Select image, resize them to the correct format and select label from training data\n",
    "X_train = X_train.reshape(-1,28,28,1) / 255\n",
    "X_val = X_val.reshape(-1,28,28,1) / 255\n",
    "\n",
    "# One-hot encoding of labels since we want to classifiy images into different units of a single vector.\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "y = np.concatenate((y_train, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(convolutional.Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "# model.add(convolutional.MaxPooling2D())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(convolutional.Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "# model.add(convolutional.MaxPooling2D())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=15, batch_size=10, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train/val accuracy/loss\n",
    "# _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "# _, test_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "# print('Train: %.3f, Validation: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# plt.figure(figsize=(20, 7))\n",
    "# plt.subplot(1,2,1)\n",
    "# train_loss_plot, = plt.plot(range(1, len(loss)+1), loss, label='Train Loss')\n",
    "# val_loss_plot, = plt.plot(range(1, len(val_loss)+1), val_loss, label='Validation Loss')\n",
    "# _ = plt.legend(handles=[train_loss_plot, val_loss_plot])\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# train_acc_plot, = plt.plot(range(1, len(acc)+1), acc, label='Training accuracy')\n",
    "# val_acc_plot, = plt.plot(range(1, len(val_acc)+1), val_acc, label='Validation accuracy')\n",
    "# _ = plt.legend(handles=[train_acc_plot, val_acc_plot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = {'0':'T-shirt/top', '1':'Trouser', '2':'Pullover',\n",
    "#          '3':'Dress', '4':'Coat', '5':'Sandal',\n",
    "#          '6':'Shirt', '7':'Sneaker', '8':'Bag', '9':'Ankle boot'}\n",
    "\n",
    "# gt = [np.argmax(k, axis=None, out=None) for k in y_val]\n",
    "# pred = [np.argmax(k, axis=None, out=None) for k in model.predict(X_val, verbose=0)]\n",
    "# res = confusion_matrix(gt, pred)\n",
    "\n",
    "# confusion = pd.DataFrame(res, index = label.values(), columns = label.values())\n",
    "# sns.heatmap(confusion, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong = np.where(np.subtract(gt,pred)!=0)[0]\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# for idx, f in enumerate(wrong[:100]):\n",
    "#     img = np.squeeze(X_val[f]*255)\n",
    "#     plt.subplot(10,10,idx+1)\n",
    "#     plt.subplots_adjust(hspace=0.5)\n",
    "#     plt.imshow(img, cmap='gray', aspect='equal')\n",
    "#     plt.title('%s - %s'%(label.get(str(gt[f])),label.get(str(pred[f]))))\n",
    "#     plt.xticks([]), plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'batch_size': [10, 50, 100],\n",
    "#           'dropout': [0, 0.25, 0.5],\n",
    "#           'kernel_size':[(3,3),(5,5)],\n",
    "#           'activation':['relu', 'elu']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_grid_search(X_train, y_train, X_val, y_val, params):\n",
    "    \n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, params['kernal_size'], input_shape=(28, 28, 1), activation=params['activation']))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(params['dropout']))\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, params['kernal_size'], activation=params['activation']))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(params['dropout']))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    \n",
    "#     model.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "#     model.fit(X_train, y_train, epochs=15, batch_size = params['batch_size'],\n",
    "#                         verbose = 0, validation_data = (X_val, y_val))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = ta.Scan(X_train, y_train, model = model_grid_search, params = params, experiment_name = 'grid_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp.data.sort_values('val_accuracy',ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'batch_size': [10, 50, 100],\n",
    "#           'dropout': [0, 0.25, 0.5],\n",
    "#           'kernel_size':[(3,3),(5,5)],\n",
    "#           'activation':['relu', 'elu'],\n",
    "#           'n_classes':[10]}\n",
    "\n",
    "# def model_random_search(kernel_size, activation, n_classes, dropout):\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, kernel_size, input_shape=(28, 28, 1), activation=activation))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(dropout))\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, kernel_size, activation=activation))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(dropout))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "#     model.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model_keras = KerasClassifier(build_fn = model_random_search, epochs=1, verbose=1)\n",
    "# random_search = RandomizedSearchCV(estimator=model_keras,\n",
    "#                                    param_distributions=params, \n",
    "#                                    verbose=20,\n",
    "#                                    n_iter=10,\n",
    "# #                                    scoring = 'accuracy',\n",
    "#                                    n_jobs=1).fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "# means = random_search.cv_results_['mean_test_score']\n",
    "# stds = random_search.cv_results_['std_test_score']\n",
    "# params = random_search.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "Compared to more simpler hyperparameter search methods like grid search and random search, Bayesian optimization is built upon Bayesian inference and Gaussian process with an attempts to find the maximum value of an unknown function as few iterations as possible. It is particularly suited for optimization of high-cost functions like hyperparameter search for deep learning model, or other situations where the balance between exploration and exploitation is important.\n",
    "\n",
    "The Bayesian Optimization package we are going to use is BayesianOptimization, which can be installed with the following command,\n",
    "\n",
    "pip install bayesian-optimization\n",
    "Firstly, we will specify the function to be optimized, in our case, hyperparameters search, the function takes a set of hyperparameters values as inputs, and output the evaluation accuracy for the Bayesian optimizer. Inside the function, a new model will be constructed with the specified hyperparameters, train for a number of epochs and evaluated against a set metrics. Every new evaluated accuracy will become a new observation for the Bayesian optimizer, which contributes to the next search hyperparameters' values. \n",
    "\n",
    "Let's create a helper function first which builds the model with various parameters.\n",
    "\n",
    "Bayesian optimization is a probabilistic model that maps the hyperparameters to a probability score on the objective function. Unlike Random Search and Hyperband models, Bayesian Optimization keeps track of its past evaluation results and uses it to build the probability model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How to Implement Bayesian Optimization from Scratch in Python\n",
    "by Jason Brownlee on October 9, 2019 in Probability\n",
    "Tweet  Share\n",
    "Last Updated on January 10, 2020\n",
    "\n",
    "In this tutorial, you will discover how to implement the Bayesian Optimization algorithm for complex optimization problems.\n",
    "\n",
    "Global optimization is a challenging problem of finding an input that results in the minimum or maximum cost of a given objective function.\n",
    "\n",
    "Typically, the form of the objective function is complex and intractable to analyze and is often non-convex, nonlinear, high dimension, noisy, and computationally expensive to evaluate.\n",
    "\n",
    "Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.\n",
    "\n",
    "Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "Global optimization is a challenging problem that involves black box and often non-convex, non-linear, noisy, and computationally expensive objective functions.\n",
    "Bayesian Optimization provides a probabilistically principled method for global optimization.\n",
    "How to implement Bayesian Optimization from scratch and how to use open-source implementations.\n",
    "Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "Update Jan/2020: Updated for changes in scikit-learn v0.22 API.\n",
    "A Gentle Introduction to Bayesian Optimization\n",
    "A Gentle Introduction to Bayesian Optimization\n",
    "Photo by Beni Arnold, some rights reserved.\n",
    "\n",
    "Tutorial Overview\n",
    "This tutorial is divided into four parts; they are:\n",
    "\n",
    "Challenge of Function Optimization\n",
    "What Is Bayesian Optimization\n",
    "How to Perform Bayesian Optimization\n",
    "Hyperparameter Tuning With Bayesian Optimization\n",
    "Challenge of Function Optimization\n",
    "Global function optimization, or function optimization for short, involves finding the minimum or maximum of an objective function.\n",
    "\n",
    "Samples are drawn from the domain and evaluated by the objective function to give a score or cost.\n",
    "\n",
    "Let’s define some common terms:\n",
    "\n",
    "Samples. One example from the domain, represented as a vector.\n",
    "Search Space: Extent of the domain from which samples can be drawn.\n",
    "Objective Function. Function that takes a sample and returns a cost.\n",
    "Cost. Numeric score for a sample calculated via the objective function.\n",
    "Samples are comprised of one or more variables generally easy to devise or create. One sample is often defined as a vector of variables with a predefined range in an n-dimensional space. This space must be sampled and explored in order to find the specific combination of variable values that result in the best cost.\n",
    "\n",
    "The cost often has units that are specific to a given domain. Optimization is often described in terms of minimizing cost, as a maximization problem can easily be transformed into a minimization problem by inverting the calculated cost. Together, the minimum and maximum of a function are referred to as the extreme of the function (or the plural extrema).\n",
    "\n",
    "The objective function is often easy to specify but can be computationally challenging to calculate or result in a noisy calculation of cost over time. The form of the objective function is unknown and is often highly nonlinear, and highly multi-dimensional defined by the number of input variables. The function is also probably non-convex. This means that local extrema may or may not be the global extrema (e.g. could be misleading and result in premature convergence), hence the name of the task as global rather than local optimization.\n",
    "\n",
    "Although little is known about the objective function, (it is known whether the minimum or the maximum cost from the function is sought), and as such, it is often referred to as a black box function and the search process as black box optimization. Further, the objective function is sometimes called an oracle given the ability to only give answers.\n",
    "\n",
    "Function optimization is a fundamental part of machine learning. Most machine learning algorithms involve the optimization of parameters (weights, coefficients, etc.) in response to training data. Optimization also refers to the process of finding the best set of hyperparameters that configure the training of a machine learning algorithm. Taking one step higher again, the selection of training data, data preparation, and machine learning algorithms themselves is also a problem of function optimization.\n",
    "\n",
    "Summary of optimization in machine learning:\n",
    "\n",
    "Algorithm Training. Optimization of model parameters.\n",
    "Algorithm Tuning. Optimization of model hyperparameters.\n",
    "Predictive Modeling. Optimization of data, data preparation, and algorithm selection.\n",
    "Many methods exist for function optimization, such as randomly sampling the variable search space, called random search, or systematically evaluating samples in a grid across the search space, called grid search.\n",
    "\n",
    "More principled methods are able to learn from sampling the space so that future samples are directed toward the parts of the search space that are most likely to contain the extrema.\n",
    "\n",
    "A directed approach to global optimization that uses probability is called Bayesian Optimization.\n",
    "\n",
    "Want to Learn Probability for Machine Learning\n",
    "Take my free 7-day email crash course now (with sample code).\n",
    "\n",
    "Click to sign-up and also get a free PDF Ebook version of the course.\n",
    "\n",
    "Download Your FREE Mini-Course\n",
    "What Is Bayesian Optimization\n",
    "Bayesian Optimization is an approach that uses Bayes Theorem to direct the search in order to find the minimum or maximum of an objective function.\n",
    "\n",
    "It is an approach that is most useful for objective functions that are complex, noisy, and/or expensive to evaluate.\n",
    "\n",
    "Bayesian optimization is a powerful strategy for finding the extrema of objective functions that are expensive to evaluate. […] It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bayesian_optimization_model(X_train, y_train, X_val, y_val, params):\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, params['kernal_size'], input_shape=(28, 28, 1), activation=params['activation']))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(params['dropout']))\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, params['kernal_size'], activation=params['activation']))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(params['dropout']))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    \n",
    "#     model.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "#     model.fit(X_train, y_train, epochs=15, batch_size = params['batch_size'],\n",
    "#                         verbose = 0, validation_data = (X_val, y_val))\n",
    "#     score = model.evaluate(eval_ds, steps=10, verbose=0)\n",
    "#     return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bayesian_optimization_model(params, n_class):\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, (3,3), input_shape=(28, 28, 1), activation=params[0]))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(params[1]))\n",
    "    \n",
    "#     model.add(convolutional.Conv2D(32, (3,3), activation=params[0]))\n",
    "#     model.add(convolutional.MaxPooling2D())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(params[1]))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(n_class, activation='softmax'))\n",
    "    \n",
    "#     model.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import skopt\n",
    "# from skopt import gbrt_minimize, gp_minimize\n",
    "# from skopt.utils import use_named_args\n",
    "# from skopt.space import Real, Categorical, Integer  \n",
    "# import tensorflow\n",
    "# from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "# # dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "# # dim_num_input_nodes = Integer(low=1, high=512, name='num_input_nodes')\n",
    "# # dim_num_dense_nodes = Integer(low=1, high=28, name='num_dense_nodes')\n",
    "\n",
    "# dim_activation = Categorical(categories=['relu','sigmoid','elu'], name='activation')\n",
    "# dim_dropout = Real(low=0, high=0.5, name='dropout_rate')\n",
    "# dim_batch_size = Integer(low=10, high=100, name='batch_size')\n",
    "# dim_kernel_size = Integer(low=3, high=9, name='kernel_size')\n",
    "\n",
    "# dimensions = [dim_activation, dim_dropout, dim_batch_size]\n",
    "# default_parameters = ['relu',0.25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fitness(X_val, y_val, params):\n",
    "\n",
    "#     model = bayesian_optimization_model(params, 10)\n",
    "#     blackbox = model.fit(x=X_train, y=y_train, epochs=15, batch_size=params[2], validation_data=(X_val, y_val))\n",
    "#     accuracy = blackbox.history['val_acc'][-1]\n",
    "#     print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "\n",
    "#     # Delete the Keras model with these hyper-parameters from memory.\n",
    "#     del model\n",
    "#     K.clear_session()\n",
    "#     tensorflow.reset_default_graph()\n",
    "    \n",
    "#     # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "#     return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# tensorflow.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp_result = gp_minimize(func=fitness, dimensions=dimensions, n_calls=12, noise=0.01, n_jobs=-1, kappa=5, x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model(gp_result.x[0],gp_result.x[1],gp_result.x[2],gp_result.x[3],gp_result.x[4],gp_result.x[5])\n",
    "# model.fit(X_train,y_train, epochs=3)\n",
    "# model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  dropout  | kernel... |\n",
      "-------------------------------------------------------------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7252  \u001b[0m | \u001b[0m 2.732   \u001b[0m | \u001b[0m 0.0998  \u001b[0m | \u001b[0m 5.479   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7119  \u001b[0m | \u001b[0m 12.11   \u001b[0m | \u001b[0m 0.27    \u001b[0m | \u001b[0m 7.432   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7362  \u001b[0m | \u001b[95m 17.4    \u001b[0m | \u001b[95m 0.08257 \u001b[0m | \u001b[95m 3.946   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7265  \u001b[0m | \u001b[0m 23.69   \u001b[0m | \u001b[0m 0.1232  \u001b[0m | \u001b[0m 4.645   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7213  \u001b[0m | \u001b[0m 35.05   \u001b[0m | \u001b[0m 0.3421  \u001b[0m | \u001b[0m 3.502   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7     \u001b[0m | \u001b[0m 15.46   \u001b[0m | \u001b[0m 0.1925  \u001b[0m | \u001b[0m 8.685   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7119  \u001b[0m | \u001b[0m 30.51   \u001b[0m | \u001b[0m 0.3762  \u001b[0m | \u001b[0m 7.572   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7134  \u001b[0m | \u001b[0m 14.36   \u001b[0m | \u001b[0m 0.3353  \u001b[0m | \u001b[0m 7.667   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7165  \u001b[0m | \u001b[0m 36.41   \u001b[0m | \u001b[0m 0.2605  \u001b[0m | \u001b[0m 6.08    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7078  \u001b[0m | \u001b[0m 41.78   \u001b[0m | \u001b[0m 0.1954  \u001b[0m | \u001b[0m 7.569   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7257  \u001b[0m | \u001b[0m 24.07   \u001b[0m | \u001b[0m 0.05579 \u001b[0m | \u001b[0m 6.82    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6955  \u001b[0m | \u001b[0m 35.76   \u001b[0m | \u001b[0m 0.2849  \u001b[0m | \u001b[0m 8.104   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6895  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6388  \u001b[0m | \u001b[0m 1.031   \u001b[0m | \u001b[0m 0.463   \u001b[0m | \u001b[0m 3.01    \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-14.38   \u001b[0m | \u001b[0m 1.463   \u001b[0m | \u001b[0m 0.4999  \u001b[0m | \u001b[0m 8.982   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7292  \u001b[0m | \u001b[0m 2.547   \u001b[0m | \u001b[0m 0.1093  \u001b[0m | \u001b[0m 3.969   \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.7399  \u001b[0m | \u001b[95m 34.62   \u001b[0m | \u001b[95m 0.3428  \u001b[0m | \u001b[95m 5.611   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6991  \u001b[0m | \u001b[0m 13.24   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 33.2    \u001b[0m | \u001b[0m 0.3113  \u001b[0m | \u001b[0m 7.504   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7174  \u001b[0m | \u001b[0m 4.391   \u001b[0m | \u001b[0m 0.2585  \u001b[0m | \u001b[0m 4.779   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.7398  \u001b[0m | \u001b[0m 31.95   \u001b[0m | \u001b[0m 0.3703  \u001b[0m | \u001b[0m 5.561   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.6866  \u001b[0m | \u001b[0m 16.6    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 6.22    \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7248  \u001b[0m | \u001b[0m 14.89   \u001b[0m | \u001b[0m 0.2069  \u001b[0m | \u001b[0m 4.545   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7243  \u001b[0m | \u001b[0m 21.64   \u001b[0m | \u001b[0m 0.06463 \u001b[0m | \u001b[0m 6.017   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7071  \u001b[0m | \u001b[0m 38.96   \u001b[0m | \u001b[0m 0.245   \u001b[0m | \u001b[0m 7.623   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7398  \u001b[0m | \u001b[0m 40.11   \u001b[0m | \u001b[0m 0.2119  \u001b[0m | \u001b[0m 5.346   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7066  \u001b[0m | \u001b[0m 37.92   \u001b[0m | \u001b[0m 0.4534  \u001b[0m | \u001b[0m 3.641   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.7219  \u001b[0m | \u001b[0m 20.39   \u001b[0m | \u001b[0m 0.3132  \u001b[0m | \u001b[0m 3.548   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7165  \u001b[0m | \u001b[0m 19.19   \u001b[0m | \u001b[0m 0.436   \u001b[0m | \u001b[0m 6.122   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.7324  \u001b[0m | \u001b[0m 27.17   \u001b[0m | \u001b[0m 0.2364  \u001b[0m | \u001b[0m 6.016   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.7168  \u001b[0m | \u001b[0m 29.4    \u001b[0m | \u001b[0m 0.06804 \u001b[0m | \u001b[0m 4.441   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.6963  \u001b[0m | \u001b[0m 26.53   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.194   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.6921  \u001b[0m | \u001b[0m 27.26   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.6836  \u001b[0m | \u001b[0m 43.23   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.781   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.6974  \u001b[0m | \u001b[0m 46.84   \u001b[0m | \u001b[0m 0.423   \u001b[0m | \u001b[0m 4.025   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.6726  \u001b[0m | \u001b[0m 45.37   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.058   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.7073  \u001b[0m | \u001b[0m 49.02   \u001b[0m | \u001b[0m 0.07784 \u001b[0m | \u001b[0m 6.225   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.7085  \u001b[0m | \u001b[0m 20.96   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[95m 39      \u001b[0m | \u001b[95m 0.7428  \u001b[0m | \u001b[95m 11.58   \u001b[0m | \u001b[95m 0.2164  \u001b[0m | \u001b[95m 3.91    \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.699   \u001b[0m | \u001b[0m 8.985   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.656   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.7166  \u001b[0m | \u001b[0m 8.127   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.6983  \u001b[0m | \u001b[0m 47.95   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.6844  \u001b[0m | \u001b[0m 9.424   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.6929  \u001b[0m | \u001b[0m 18.3    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.7025  \u001b[0m | \u001b[0m 31.75   \u001b[0m | \u001b[0m 0.4862  \u001b[0m | \u001b[0m 3.014   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.7214  \u001b[0m | \u001b[0m 41.45   \u001b[0m | \u001b[0m 0.3634  \u001b[0m | \u001b[0m 3.005   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.7069  \u001b[0m | \u001b[0m 24.24   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.7051  \u001b[0m | \u001b[0m 44.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.6911  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 0.2076  \u001b[0m | \u001b[0m 8.899   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.7242  \u001b[0m | \u001b[0m 44.45   \u001b[0m | \u001b[0m 0.02324 \u001b[0m | \u001b[0m 3.079   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.6915  \u001b[0m | \u001b[0m 7.195   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.235   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.7225  \u001b[0m | \u001b[0m 13.85   \u001b[0m | \u001b[0m 0.4157  \u001b[0m | \u001b[0m 3.006   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.7158  \u001b[0m | \u001b[0m 5.793   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.7159  \u001b[0m | \u001b[0m 22.62   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.7154  \u001b[0m | \u001b[0m 40.18   \u001b[0m | \u001b[0m 0.1095  \u001b[0m | \u001b[0m 8.998   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.6953  \u001b[0m | \u001b[0m 6.896   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.941   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.6894  \u001b[0m | \u001b[0m 31.78   \u001b[0m | \u001b[0m 0.02551 \u001b[0m | \u001b[0m 8.969   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.7017  \u001b[0m | \u001b[0m 37.55   \u001b[0m | \u001b[0m 0.3532  \u001b[0m | \u001b[0m 8.996   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.7091  \u001b[0m | \u001b[0m 12.95   \u001b[0m | \u001b[0m 0.4941  \u001b[0m | \u001b[0m 5.567   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.6841  \u001b[0m | \u001b[0m 29.33   \u001b[0m | \u001b[0m 0.422   \u001b[0m | \u001b[0m 8.96    \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.6858  \u001b[0m | \u001b[0m 28.75   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.7344  \u001b[0m | \u001b[0m 47.9    \u001b[0m | \u001b[0m 0.02318 \u001b[0m | \u001b[0m 3.001   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.6521  \u001b[0m | \u001b[0m 45.85   \u001b[0m | \u001b[0m 0.004269\u001b[0m | \u001b[0m 8.98    \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 39.55   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.7026  \u001b[0m | \u001b[0m 10.11   \u001b[0m | \u001b[0m 0.483   \u001b[0m | \u001b[0m 3.078   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.6922  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 4.793   \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.7266  \u001b[0m | \u001b[0m 15.85   \u001b[0m | \u001b[0m 0.3877  \u001b[0m | \u001b[0m 3.009   \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.6901  \u001b[0m | \u001b[0m 9.936   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.221   \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.6746  \u001b[0m | \u001b[0m 34.03   \u001b[0m | \u001b[0m 0.000588\u001b[0m | \u001b[0m 8.975   \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.7042  \u001b[0m | \u001b[0m 24.64   \u001b[0m | \u001b[0m 0.4465  \u001b[0m | \u001b[0m 3.005   \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.6918  \u001b[0m | \u001b[0m 25.71   \u001b[0m | \u001b[0m 0.4942  \u001b[0m | \u001b[0m 7.328   \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.6946  \u001b[0m | \u001b[0m 11.26   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.703   \u001b[0m | \u001b[0m 18.63   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.6911  \u001b[0m | \u001b[0m 47.36   \u001b[0m | \u001b[0m 0.4933  \u001b[0m | \u001b[0m 6.828   \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.6977  \u001b[0m | \u001b[0m 28.69   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 6.907   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.7267  \u001b[0m | \u001b[0m 33.69   \u001b[0m | \u001b[0m 0.0271  \u001b[0m | \u001b[0m 3.014   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.689   \u001b[0m | \u001b[0m 22.47   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.201   \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.7149  \u001b[0m | \u001b[0m 25.36   \u001b[0m | \u001b[0m 0.01761 \u001b[0m | \u001b[0m 5.05    \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.6664  \u001b[0m | \u001b[0m 19.98   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.656   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.7169  \u001b[0m | \u001b[0m 38.26   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.491   \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.6986  \u001b[0m | \u001b[0m 42.18   \u001b[0m | \u001b[0m 0.06302 \u001b[0m | \u001b[0m 8.998   \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.6772  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.145   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.7197  \u001b[0m | \u001b[0m 45.19   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.041   \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.7046  \u001b[0m | \u001b[0m 43.41   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 6.714   \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.6939  \u001b[0m | \u001b[0m 17.47   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.681   \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.6829  \u001b[0m | \u001b[0m 7.887   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.7163  \u001b[0m | \u001b[0m 10.77   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.373   \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.7314  \u001b[0m | \u001b[0m 36.36   \u001b[0m | \u001b[0m 0.03081 \u001b[0m | \u001b[0m 3.063   \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.6902  \u001b[0m | \u001b[0m 45.95   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.7073  \u001b[0m | \u001b[0m 41.73   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 4.904   \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 0.6994  \u001b[0m | \u001b[0m 32.95   \u001b[0m | \u001b[0m 0.03547 \u001b[0m | \u001b[0m 4.355   \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.6905  \u001b[0m | \u001b[0m 19.23   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 4.603   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 0.6802  \u001b[0m | \u001b[0m 8.338   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.587   \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.6913  \u001b[0m | \u001b[0m 48.41   \u001b[0m | \u001b[0m 0.4749  \u001b[0m | \u001b[0m 4.561   \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.6981  \u001b[0m | \u001b[0m 27.77   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.546   \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.6911  \u001b[0m | \u001b[0m 21.91   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.437   \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.6975  \u001b[0m | \u001b[0m 3.257   \u001b[0m | \u001b[0m 0.02675 \u001b[0m | \u001b[0m 4.84    \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.615   \u001b[0m | \u001b[0m 1.131   \u001b[0m | \u001b[0m 0.2315  \u001b[0m | \u001b[0m 4.352   \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.7112  \u001b[0m | \u001b[0m 5.71    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.877   \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.7025  \u001b[0m | \u001b[0m 30.27   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.981   \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 0.6884  \u001b[0m | \u001b[0m 25.56   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.0     \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 0.686   \u001b[0m | \u001b[0m 2.376   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.751   \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m 0.7223  \u001b[0m | \u001b[0m 30.17   \u001b[0m | \u001b[0m 0.02212 \u001b[0m | \u001b[0m 3.069   \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m 0.7152  \u001b[0m | \u001b[0m 36.04   \u001b[0m | \u001b[0m 0.01856 \u001b[0m | \u001b[0m 4.51    \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m 0.7152  \u001b[0m | \u001b[0m 3.977   \u001b[0m | \u001b[0m 0.4256  \u001b[0m | \u001b[0m 3.04    \u001b[0m |\n",
      "| \u001b[0m 106     \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 48.59   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.911   \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m 0.7336  \u001b[0m | \u001b[0m 12.14   \u001b[0m | \u001b[0m 0.03181 \u001b[0m | \u001b[0m 3.011   \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m 0.7134  \u001b[0m | \u001b[0m 22.67   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 6.82    \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m 0.7263  \u001b[0m | \u001b[0m 42.89   \u001b[0m | \u001b[0m 0.009408\u001b[0m | \u001b[0m 3.022   \u001b[0m |\n",
      "| \u001b[0m 110     \u001b[0m | \u001b[0m 0.6908  \u001b[0m | \u001b[0m 16.53   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.945   \u001b[0m |\n",
      "| \u001b[0m 111     \u001b[0m | \u001b[0m 0.6817  \u001b[0m | \u001b[0m 37.32   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.479   \u001b[0m |\n",
      "| \u001b[0m 112     \u001b[0m | \u001b[0m 0.6949  \u001b[0m | \u001b[0m 8.938   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.25    \u001b[0m |\n",
      "=============================================================\n",
      "nbest result: {'params': {'batch_size': 11.576932738582627, 'dropout': 0.21643685134436597, 'kernel_size': 3.9101315061390367}, 'target': 0.7427879032944595}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Input, Flatten, Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# import confusion_matrix_plot\n",
    "\n",
    "early_stop_epochs = 10\n",
    "learning_rate_epochs = 5\n",
    "\n",
    "# parameters that change for each iteration that must be saved\n",
    "list_early_stop_epochs = []\n",
    "list_validation_loss = []\n",
    "list_saved_model_name = []\n",
    "\n",
    "def bayesian_optimization(kernel_size, batch_size, dropout):\n",
    "    ker = int(kernel_size)\n",
    "    batch = int(batch_size)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(convolutional.Conv2D(32, (ker,ker), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(convolutional.MaxPooling2D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(convolutional.Conv2D(32, (ker,ker), activation='relu'))\n",
    "    model.add(convolutional.MaxPooling2D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00025), metrics=['accuracy'])\n",
    "    callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs),                     \n",
    "                      ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n",
    "                                        patience=learning_rate_epochs, \n",
    "                                        verbose=0, mode='auto', min_lr=1.0e-6),\n",
    "                      ModelCheckpoint(filepath='model_ker_%s_batch_%s_drop_%s.h5'%(kernel_size,batch_size,dropout),\n",
    "                                      monitor='val_loss', save_best_only=True)]\n",
    "    \n",
    "    history = model.fit(X,y,batch_size=batch,\n",
    "                        epochs=15,verbose=0,validation_split=0.25, \n",
    "                        shuffle=False,callbacks=callbacks_list)\n",
    "\n",
    "    # record actual best epochs and valid loss here, added to bayes opt parameter df below\n",
    "    list_early_stop_epochs.append(len(history.history['val_loss']) - early_stop_epochs)\n",
    "\n",
    "    validation_loss = np.min(history.history['val_loss'])  # h.history['val_loss']\n",
    "    list_validation_loss.append(validation_loss)\n",
    "    list_saved_model_name.append('model_ker_%s_batch_%s_drop_%s.h5'%(kernel_size,batch_size,dropout))\n",
    "\n",
    "    # bayes opt is a maximization algorithm, to minimize validation_loss, return 1-this\n",
    "    bayes_score = 1.0 - validation_loss\n",
    "    return bayes_score\n",
    "\n",
    "params_opt = {'kernel_size':(3, 9),\n",
    "              'batch_size':(1, 50),\n",
    "              'dropout': (0, 0.5)}\n",
    "\n",
    "optimizer = BayesianOptimization(f=bayesian_optimization, pbounds=params_opt, verbose=2)\n",
    "optimizer.maximize(init_points=12, n_iter=100)\n",
    "\n",
    "print('nbest result:', optimizer.max)\n",
    "\n",
    "list_dfs = []\n",
    "counter = 0\n",
    "for result in optimizer.res:\n",
    "    df_temp = pd.DataFrame.from_dict(data=result['params'], orient='index', columns=['trial' + str(counter)]).T\n",
    "    df_temp['bayes opt error'] = result['target']\n",
    "    df_temp['epochs'] = list_early_stop_epochs[counter]\n",
    "    df_temp['validation_loss'] = list_validation_loss[counter]\n",
    "    df_temp['model_name'] = list_saved_model_name[counter]\n",
    "    list_dfs.append(df_temp)\n",
    "    counter = counter + 1\n",
    "\n",
    "df_results = pd.concat(list_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
